# Smart Throttler Configuration
# Production-grade rate limiting for Gemini API
#
# This configuration is designed to prevent 429 RESOURCE_EXHAUSTED errors
# while maximizing throughput for multi-day pipeline runs.

smart_throttler:
  enabled: true

  # Default settings (can be overridden per model)
  defaults:
    # Maximum concurrent in-flight requests
    # This is just a CEILING - set high and forget it
    # The AIMD controller dynamically finds the optimal rate below this
    max_concurrency: 100

    # Admitted requests per second (AIMD-controlled)
    # The controller will adjust this dynamically based on 429 rate
    # - Starts at initial_admitted_rps
    # - Increases slowly when healthy (additive_increase_per_sec)
    # - Decreases by 30% on 429 (multiplicative_decrease_factor)
    initial_admitted_rps: 2.0
    min_admitted_rps: 0.1
    max_admitted_rps: 20.0

    # Target 429 rate - keep this near zero
    # Controller will reduce rate if 429s exceed this threshold
    target_429_rate: 0.002

    # Adaptive controller settings (AIMD algorithm)
    controller:
      # How fast to increase rate when healthy (RPS per second)
      additive_increase_per_sec: 0.02
      # How much to decrease rate on 429 (multiplicative factor)
      multiplicative_decrease_factor: 0.7
      # Evaluation window for rate decisions
      evaluate_window_s: 15
      # Latency-based throttling
      latency_guard:
        # If p95 latency exceeds this, reduce rate
        p95_ms_threshold: 8000
        # Decrease factor for latency issues (gentler than 429)
        latency_decrease_factor: 0.8

    # Request pacing (token bucket)
    pacing:
      # Allow short bursts up to this multiple of admitted rate
      burst_allowance: 2.0

    # Token budget limiting (prevents large steps from dominating)
    token_budget:
      enabled: true
      # Start throttling at this level
      soft_tokens_per_minute: 200000
      # Hard reject at this level
      hard_tokens_per_minute: 240000

  # Priority class configurations
  # Higher weight = more slots in weighted fair queueing
  priorities:
    interactive:
      weight: 5
      max_queue: 200
      deadline_s: 10
    standard:
      weight: 2
      max_queue: 20000
      deadline_s: 600
    background:
      weight: 1
      max_queue: 500000
      deadline_s: 7200

  # Flow ordering for chained workflows
  flows:
    # Max concurrent requests per flow_id
    # Set to 1 to preserve strict ordering within a chain
    max_in_flight_per_flow: 1

  # Retry policy
  # Retries are the "emergency brake" - they inform the controller
  retries:
    max_attempts: 6
    base_delay_ms: 250
    max_delay_ms: 30000
    jitter: true

  # Response cache (for eval reruns)
  cache:
    enabled: false
    ttl_s: 86400  # 24 hours
    max_entries: 50000

  # Step profiles for token estimation
  # These help the token budget limiter make accurate decisions
  # The variance_factor adds a safety buffer to estimates
  step_profiles:
    filtration:
      initial_estimated_prompt_tokens: 2800
      variance_factor: 1.2
    bronze:
      initial_estimated_prompt_tokens: 3000
      variance_factor: 1.2
    verb_noun_noun:
      initial_estimated_prompt_tokens: 12000
      variance_factor: 1.3
    # Add more steps as needed:
    # silver:
    #   initial_estimated_prompt_tokens: 5000
    #   variance_factor: 1.2

  # Model-specific overrides
  # Use these to tune settings for specific models
  model_overrides:
    gemini_pro_3:
      max_concurrency: 12
      initial_admitted_rps: 0.8
      max_admitted_rps: 6.0
    # gemini_flash_3:
    #   max_concurrency: 20
    #   initial_admitted_rps: 2.0
    #   max_admitted_rps: 15.0
